{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment analysis feature\n",
    "\n",
    "We have 5 different sentiment:\n",
    "\n",
    "__2 from packages__\n",
    "- TextBlob\n",
    "- VaderSentiment\n",
    "\n",
    "__3 word sentiment libraries__\n",
    "- AFINN-111\n",
    "- SentStrength\n",
    "- ANEW\n",
    "\n",
    "Before I started with this function we created dictionaries (in a json file) from the word sentiment libraries. As AFINN and SentStrength both scored words on a -2 to +2 scale I could combine those. If a word was scored in both libraries I chose SentStrength over AFINN's score.\n",
    "\n",
    "ANEW scored sentiment on 3 levels: Valence, Dominance and Arousal:\n",
    "- Valence how positive or negative a word is (1.25 - 8.82)\n",
    "- Dominance how important the word is to the sentence\n",
    "- Arousal how much impact a word has on the person reading it\n",
    "\n",
    "To get to positive/negative sentiment I transformed the valence to a -2 till +2 scale value. Then multiplied valence with the sum of dominance and arousal (the effect of the word for the sentence) $Valence * (Arousal + Dominance)$.\n",
    "\n",
    "The functions clean the words in a tweet, make it lowercase and check it for value on the dictionary. The sum of all values of found words in a tweet make the tweet's sentiment score.\n",
    "\n",
    "After the sentiment we can also look for extreme sentiment, we do this based on extreme values within the df.\n",
    "\n",
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import json\n",
    "\n",
    "from sklearn.model_selection import train_test_split #We need this to split the data\n",
    "from sklearn.preprocessing import normalize #get the function needed to normalize our data.\n",
    "from sklearn.neighbors import KNeighborsClassifier #the object class we need\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from textblob import TextBlob, Word, Blobber\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_file(filename):\n",
    "    with open(filename) as json_file:\n",
    "        return json.load(json_file)\n",
    "\n",
    "def RT(tweet):\n",
    "    if ('RT @' in tweet):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def clean_words(word):\n",
    "    word = word.lower()\n",
    "    word = word.replace(\",\", \"\")\n",
    "    word = word.replace(\".\", \"\")\n",
    "    word = word.replace(\":\", \"\")\n",
    "    word = word.replace(\"?\", \"\")\n",
    "    word = word.replace(\"#\", \"\")\n",
    "    word = word.replace(\"(\", \"\")\n",
    "    word = word.replace(\")\", \"\")\n",
    "    word = word.replace(\"!\", \"\")\n",
    "    word = word.replace(\"'\", \"\")\n",
    "    word = word.replace(\";\", \"\")\n",
    "    word = word.replace(\"&\", \"\")\n",
    "    word = word.replace(\"'\", \"\")\n",
    "    return word\n",
    "\n",
    "def add_sentiment(tweet):\n",
    "    #open the dictionary with the sentiment and set default sentiment to 0 (neutral)\n",
    "    words = open_file('sentiment_dict.json')\n",
    "    sentiment = 0\n",
    "    \n",
    "    #for each word in the tweet\n",
    "    for w in tweet.split():\n",
    "        #clean the word, remove hashtags, punctuation, etc. and make it lower case\n",
    "        w = clean_words(w)\n",
    "        w = w.lower()\n",
    "    \n",
    "        #loop through dictionary and see if we can find a match\n",
    "        for word in words:\n",
    "            if word == w:\n",
    "                #if we have a match add this sentiment to the total amount\n",
    "                sentiment += words[w]\n",
    "            else: pass\n",
    "\n",
    "    #after all words are matched return total sum of sentiment\n",
    "    return sentiment\n",
    "\n",
    "def add_anew_sentiment(tweet):\n",
    "    #open the dictionary with the sentiment and set default sentiment to 0 (neutral)\n",
    "    words = open_file('anew_sentiment_dict.json')\n",
    "    sentiment = 0\n",
    "    \n",
    "    #for each word in the tweet\n",
    "    for w in tweet.split():\n",
    "        #clean the word, remove hashtags, punctuation, etc. and make it lower case\n",
    "        w = clean_words(w)\n",
    "        w = w.lower()\n",
    "    \n",
    "        #loop through dictionary and see if we can find a match\n",
    "        for word in words:\n",
    "            if word == w:\n",
    "                #if we have a match add this sentiment to the total amount\n",
    "                sentiment += words[w]\n",
    "            else: pass\n",
    "\n",
    "    #after all words are matched return total sum of sentiment\n",
    "    return sentiment\n",
    "\n",
    "def add_extreme_sentiment(df):\n",
    "    df['extreme_AFINN_SentStrength'] = np.where((df['sentiment_AFINN_SentStrength'] > 3) | (df['sentiment_AFINN_SentStrength'] < -3), 1, 0)\n",
    "    df['extreme_textblob'] = np.where((df['sentiment_textblob'] > 0.25) | (df['sentiment_textblob'] < -0.25), 1, 0)\n",
    "    df['extreme_ANEW'] = np.where((df['sentiment_ANEW'] > 20) | (df['sentiment_ANEW'] < -19), 1, 0)\n",
    "    return df\n",
    "\n",
    "def add_vader_sentiment(df):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "    df['compound'] = [analyzer.polarity_scores(x)['compound'] for x in df['text']]\n",
    "    df['neg'] = [analyzer.polarity_scores(x)['neg'] for x in df['text']]\n",
    "    df['neu'] = [analyzer.polarity_scores(x)['neu'] for x in df['text']]\n",
    "    df['pos'] = [analyzer.polarity_scores(x)['pos'] for x in df['text']]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1161040537207463936</th>\n",
       "      <td>'RT @SenJeffMerkley: The Endangered Species Ac...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1176360756239118342</th>\n",
       "      <td>'RT @LindseyGrahamSC: Interesting concept -- i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1099036648573145088</th>\n",
       "      <td>'RT @RealJamesWoods: #BuildTheWall #DeportThem...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1092915693203480577</th>\n",
       "      <td>'RT @PatriotJackiB: Why would the MEXICAN GOV’...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1149038450668187654</th>\n",
       "      <td>'RT @TheOnion: Sweden Announces Plan To Get 10...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  text  label\n",
       "tweet_id                                                                     \n",
       "1161040537207463936  'RT @SenJeffMerkley: The Endangered Species Ac...      1\n",
       "1176360756239118342  'RT @LindseyGrahamSC: Interesting concept -- i...      1\n",
       "1099036648573145088  'RT @RealJamesWoods: #BuildTheWall #DeportThem...      0\n",
       "1092915693203480577  'RT @PatriotJackiB: Why would the MEXICAN GOV’...      0\n",
       "1149038450668187654  'RT @TheOnion: Sweden Announces Plan To Get 10...      0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('tweets_labeled.csv', index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No Retweets\n",
    "Apply only if you don't want RT's in your dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['RT'] = df['text'].apply(RT)\n",
    "df = df[df.RT == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply functions to add sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment_textblob'] = df['text'].apply(lambda x: TextBlob(x).sentiment[0])\n",
    "df['sentiment_AFINN_SentStrength'] = df['text'].apply(add_sentiment)\n",
    "df['sentiment_ANEW'] = df['text'].apply(add_anew_sentiment)\n",
    "df = add_vader_sentiment(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply functions for extreme sentiment \n",
    "Isn't written for Vader yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = add_extreme_sentiment(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>RT</th>\n",
       "      <th>sentiment_textblob</th>\n",
       "      <th>sentiment_AFINN_SentStrength</th>\n",
       "      <th>sentiment_ANEW</th>\n",
       "      <th>compound</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>extreme_AFINN_SentStrength</th>\n",
       "      <th>extreme_textblob</th>\n",
       "      <th>extreme_ANEW</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1081722778125062144</th>\n",
       "      <td>'Planned Parenthood Erects Billboards Urging W...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>-9.98</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1158761795739217921</th>\n",
       "      <td>'https://t.co/MvrznF1fWVWhoever obstructing th...</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-2</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.6808</td>\n",
       "      <td>0.318</td>\n",
       "      <td>0.682</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1095142095621365760</th>\n",
       "      <td>'CAIR Backs Ilhan Omar's 'Legitimate Criticism...</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-0.4767</td>\n",
       "      <td>0.291</td>\n",
       "      <td>0.709</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1137856356818595841</th>\n",
       "      <td>'@nopasa @cathmckenna https://t.co/ldEruis5Js'</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1090272871958695936</th>\n",
       "      <td>'Not suprised! ! https://t.co/PHc6lTQ0wl'</td>\n",
       "      <td>0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                  text  label  \\\n",
       "tweet_id                                                                        \n",
       "1081722778125062144  'Planned Parenthood Erects Billboards Urging W...      0   \n",
       "1158761795739217921  'https://t.co/MvrznF1fWVWhoever obstructing th...      1   \n",
       "1095142095621365760  'CAIR Backs Ilhan Omar's 'Legitimate Criticism...      0   \n",
       "1137856356818595841     '@nopasa @cathmckenna https://t.co/ldEruis5Js'      0   \n",
       "1090272871958695936          'Not suprised! ! https://t.co/PHc6lTQ0wl'      0   \n",
       "\n",
       "                        RT  sentiment_textblob  sentiment_AFINN_SentStrength  \\\n",
       "tweet_id                                                                       \n",
       "1081722778125062144  False                 0.0                             0   \n",
       "1158761795739217921  False                 0.0                            -2   \n",
       "1095142095621365760  False                 0.0                            -1   \n",
       "1137856356818595841  False                 0.0                             0   \n",
       "1090272871958695936  False                 0.0                             0   \n",
       "\n",
       "                     sentiment_ANEW  compound    neg    neu  pos  \\\n",
       "tweet_id                                                           \n",
       "1081722778125062144           -9.98    0.0000  0.000  1.000  0.0   \n",
       "1158761795739217921            0.00   -0.6808  0.318  0.682  0.0   \n",
       "1095142095621365760            0.00   -0.4767  0.291  0.709  0.0   \n",
       "1137856356818595841            0.00    0.0000  0.000  1.000  0.0   \n",
       "1090272871958695936            0.00    0.0000  0.000  1.000  0.0   \n",
       "\n",
       "                     extreme_AFINN_SentStrength  extreme_textblob  \\\n",
       "tweet_id                                                            \n",
       "1081722778125062144                           0                 0   \n",
       "1158761795739217921                           0                 0   \n",
       "1095142095621365760                           0                 0   \n",
       "1137856356818595841                           0                 0   \n",
       "1090272871958695936                           0                 0   \n",
       "\n",
       "                     extreme_ANEW  \n",
       "tweet_id                           \n",
       "1081722778125062144             0  \n",
       "1158761795739217921             0  \n",
       "1095142095621365760             0  \n",
       "1137856356818595841             0  \n",
       "1090272871958695936             0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check out correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>RT</th>\n",
       "      <th>sentiment_textblob</th>\n",
       "      <th>sentiment_AFINN_SentStrength</th>\n",
       "      <th>sentiment_ANEW</th>\n",
       "      <th>compound</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "      <th>extreme_AFINN_SentStrength</th>\n",
       "      <th>extreme_textblob</th>\n",
       "      <th>extreme_ANEW</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.039046</td>\n",
       "      <td>-0.047524</td>\n",
       "      <td>0.035705</td>\n",
       "      <td>-0.010043</td>\n",
       "      <td>0.016187</td>\n",
       "      <td>-0.012116</td>\n",
       "      <td>-0.001418</td>\n",
       "      <td>0.020305</td>\n",
       "      <td>-0.037341</td>\n",
       "      <td>0.023894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RT</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment_textblob</th>\n",
       "      <td>0.039046</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.386611</td>\n",
       "      <td>0.205084</td>\n",
       "      <td>0.394417</td>\n",
       "      <td>-0.300692</td>\n",
       "      <td>-0.016245</td>\n",
       "      <td>0.356030</td>\n",
       "      <td>-0.086154</td>\n",
       "      <td>0.098845</td>\n",
       "      <td>0.066466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment_AFINN_SentStrength</th>\n",
       "      <td>-0.047524</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.386611</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.390974</td>\n",
       "      <td>0.762500</td>\n",
       "      <td>-0.609120</td>\n",
       "      <td>0.169768</td>\n",
       "      <td>0.444164</td>\n",
       "      <td>-0.456032</td>\n",
       "      <td>-0.022957</td>\n",
       "      <td>-0.048130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sentiment_ANEW</th>\n",
       "      <td>0.035705</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.205084</td>\n",
       "      <td>0.390974</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.350701</td>\n",
       "      <td>-0.236137</td>\n",
       "      <td>0.022178</td>\n",
       "      <td>0.231832</td>\n",
       "      <td>-0.149907</td>\n",
       "      <td>0.036263</td>\n",
       "      <td>0.443821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compound</th>\n",
       "      <td>-0.010043</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.394417</td>\n",
       "      <td>0.762500</td>\n",
       "      <td>0.350701</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.746319</td>\n",
       "      <td>0.148831</td>\n",
       "      <td>0.625106</td>\n",
       "      <td>-0.305776</td>\n",
       "      <td>-0.018894</td>\n",
       "      <td>0.038304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neg</th>\n",
       "      <td>0.016187</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.300692</td>\n",
       "      <td>-0.609120</td>\n",
       "      <td>-0.236137</td>\n",
       "      <td>-0.746319</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.692309</td>\n",
       "      <td>-0.163752</td>\n",
       "      <td>0.413045</td>\n",
       "      <td>0.205121</td>\n",
       "      <td>0.088974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neu</th>\n",
       "      <td>-0.012116</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.016245</td>\n",
       "      <td>0.169768</td>\n",
       "      <td>0.022178</td>\n",
       "      <td>0.148831</td>\n",
       "      <td>-0.692309</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.598493</td>\n",
       "      <td>-0.376270</td>\n",
       "      <td>-0.335124</td>\n",
       "      <td>-0.198797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos</th>\n",
       "      <td>-0.001418</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.356030</td>\n",
       "      <td>0.444164</td>\n",
       "      <td>0.231832</td>\n",
       "      <td>0.625106</td>\n",
       "      <td>-0.163752</td>\n",
       "      <td>-0.598493</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.055860</td>\n",
       "      <td>0.230416</td>\n",
       "      <td>0.173016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>extreme_AFINN_SentStrength</th>\n",
       "      <td>0.020305</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.086154</td>\n",
       "      <td>-0.456032</td>\n",
       "      <td>-0.149907</td>\n",
       "      <td>-0.305776</td>\n",
       "      <td>0.413045</td>\n",
       "      <td>-0.376270</td>\n",
       "      <td>0.055860</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.196126</td>\n",
       "      <td>0.193516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>extreme_textblob</th>\n",
       "      <td>-0.037341</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.098845</td>\n",
       "      <td>-0.022957</td>\n",
       "      <td>0.036263</td>\n",
       "      <td>-0.018894</td>\n",
       "      <td>0.205121</td>\n",
       "      <td>-0.335124</td>\n",
       "      <td>0.230416</td>\n",
       "      <td>0.196126</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.147546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>extreme_ANEW</th>\n",
       "      <td>0.023894</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.066466</td>\n",
       "      <td>-0.048130</td>\n",
       "      <td>0.443821</td>\n",
       "      <td>0.038304</td>\n",
       "      <td>0.088974</td>\n",
       "      <td>-0.198797</td>\n",
       "      <td>0.173016</td>\n",
       "      <td>0.193516</td>\n",
       "      <td>0.147546</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 label  RT  sentiment_textblob  \\\n",
       "label                         1.000000 NaN            0.039046   \n",
       "RT                                 NaN NaN                 NaN   \n",
       "sentiment_textblob            0.039046 NaN            1.000000   \n",
       "sentiment_AFINN_SentStrength -0.047524 NaN            0.386611   \n",
       "sentiment_ANEW                0.035705 NaN            0.205084   \n",
       "compound                     -0.010043 NaN            0.394417   \n",
       "neg                           0.016187 NaN           -0.300692   \n",
       "neu                          -0.012116 NaN           -0.016245   \n",
       "pos                          -0.001418 NaN            0.356030   \n",
       "extreme_AFINN_SentStrength    0.020305 NaN           -0.086154   \n",
       "extreme_textblob             -0.037341 NaN            0.098845   \n",
       "extreme_ANEW                  0.023894 NaN            0.066466   \n",
       "\n",
       "                              sentiment_AFINN_SentStrength  sentiment_ANEW  \\\n",
       "label                                            -0.047524        0.035705   \n",
       "RT                                                     NaN             NaN   \n",
       "sentiment_textblob                                0.386611        0.205084   \n",
       "sentiment_AFINN_SentStrength                      1.000000        0.390974   \n",
       "sentiment_ANEW                                    0.390974        1.000000   \n",
       "compound                                          0.762500        0.350701   \n",
       "neg                                              -0.609120       -0.236137   \n",
       "neu                                               0.169768        0.022178   \n",
       "pos                                               0.444164        0.231832   \n",
       "extreme_AFINN_SentStrength                       -0.456032       -0.149907   \n",
       "extreme_textblob                                 -0.022957        0.036263   \n",
       "extreme_ANEW                                     -0.048130        0.443821   \n",
       "\n",
       "                              compound       neg       neu       pos  \\\n",
       "label                        -0.010043  0.016187 -0.012116 -0.001418   \n",
       "RT                                 NaN       NaN       NaN       NaN   \n",
       "sentiment_textblob            0.394417 -0.300692 -0.016245  0.356030   \n",
       "sentiment_AFINN_SentStrength  0.762500 -0.609120  0.169768  0.444164   \n",
       "sentiment_ANEW                0.350701 -0.236137  0.022178  0.231832   \n",
       "compound                      1.000000 -0.746319  0.148831  0.625106   \n",
       "neg                          -0.746319  1.000000 -0.692309 -0.163752   \n",
       "neu                           0.148831 -0.692309  1.000000 -0.598493   \n",
       "pos                           0.625106 -0.163752 -0.598493  1.000000   \n",
       "extreme_AFINN_SentStrength   -0.305776  0.413045 -0.376270  0.055860   \n",
       "extreme_textblob             -0.018894  0.205121 -0.335124  0.230416   \n",
       "extreme_ANEW                  0.038304  0.088974 -0.198797  0.173016   \n",
       "\n",
       "                              extreme_AFINN_SentStrength  extreme_textblob  \\\n",
       "label                                           0.020305         -0.037341   \n",
       "RT                                                   NaN               NaN   \n",
       "sentiment_textblob                             -0.086154          0.098845   \n",
       "sentiment_AFINN_SentStrength                   -0.456032         -0.022957   \n",
       "sentiment_ANEW                                 -0.149907          0.036263   \n",
       "compound                                       -0.305776         -0.018894   \n",
       "neg                                             0.413045          0.205121   \n",
       "neu                                            -0.376270         -0.335124   \n",
       "pos                                             0.055860          0.230416   \n",
       "extreme_AFINN_SentStrength                      1.000000          0.196126   \n",
       "extreme_textblob                                0.196126          1.000000   \n",
       "extreme_ANEW                                    0.193516          0.147546   \n",
       "\n",
       "                              extreme_ANEW  \n",
       "label                             0.023894  \n",
       "RT                                     NaN  \n",
       "sentiment_textblob                0.066466  \n",
       "sentiment_AFINN_SentStrength     -0.048130  \n",
       "sentiment_ANEW                    0.443821  \n",
       "compound                          0.038304  \n",
       "neg                               0.088974  \n",
       "neu                              -0.198797  \n",
       "pos                               0.173016  \n",
       "extreme_AFINN_SentStrength        0.193516  \n",
       "extreme_textblob                  0.147546  \n",
       "extreme_ANEW                      1.000000  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr = df.corr()\n",
    "corr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
